{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8430a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cf401",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3784602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import relax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39e79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Equivalent without using __file__, assuming you're in notebooks/\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from AllocationFinder import AllocationFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce3297",
   "metadata": {},
   "source": [
    "# Relax Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439f2dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">forward</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(x, conv1_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(conv1_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
       "            conv2d: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv, lv1)\n",
       "            relu: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(conv2d)\n",
       "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(relu, conv2_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(conv2_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
       "            conv2d1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv2, lv3)\n",
       "            relu1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(conv2d1)\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> relu1\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class RelaxMnist(relax.frontend.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelaxMnist, self).__init__()\n",
    "        self.conv1 = relax.frontend.nn.Conv2D(3, 32, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        self.relu1 = relax.frontend.nn.ReLU()\n",
    "        self.conv2 = relax.frontend.nn.Conv2D(32, 64, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        self.relu2 = relax.frontend.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_shape = (1, 3, 128, 128)\n",
    "rconv_mod, rconv_params = RelaxMnist().export_tvm({\"forward\": {\"x\": relax.frontend.nn.spec.Tensor(input_shape, \"float32\")}})\n",
    "rconv_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bbd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_conv2d1_add1_relu1</span>(relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_weight: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), lv3: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute_intermediate: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        pad_temp <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>)))\n",
       "        conv2d_nchw_intermediate <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)))\n",
       "        T_add_intermediate <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(relu[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
       "                pad_temp[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>if_then_else(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i2 <span style=\"color: #008000; font-weight: bold\">and</span> v_i2 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">130</span>) <span style=\"color: #008000; font-weight: bold\">and</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i3 <span style=\"color: #008000; font-weight: bold\">and</span> v_i3 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">130</span>), relu[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> nn, ff, yy, xx, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
       "                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [nn, ff, yy, xx, rc, ry, rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx], conv2_weight[v_ff, v_rc, v_ry, v_rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">+</span> pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx] <span style=\"color: #A2F; font-weight: bold\">*</span> conv2_weight[v_ff, v_rc, v_ry, v_rx]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv3[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">+</span> lv3[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(T_add_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(compute_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                compute_intermediate[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(T_add_intermediate[v_i0, v_i1, v_i2, v_i3], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_conv2d_add_relu</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_weight: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), lv1: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute_intermediate: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        pad_temp <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>)))\n",
       "        conv2d_nchw_intermediate <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)))\n",
       "        T_add_intermediate <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
       "                pad_temp[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>if_then_else(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i2 <span style=\"color: #008000; font-weight: bold\">and</span> v_i2 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">130</span>) <span style=\"color: #008000; font-weight: bold\">and</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i3 <span style=\"color: #008000; font-weight: bold\">and</span> v_i3 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">130</span>), x[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> nn, ff, yy, xx, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
       "                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [nn, ff, yy, xx, rc, ry, rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx], conv1_weight[v_ff, v_rc, v_ry, v_rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">+</span> pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx] <span style=\"color: #A2F; font-weight: bold\">*</span> conv1_weight[v_ff, v_rc, v_ry, v_rx]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv1[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">+</span> lv1[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(T_add_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(compute_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                compute_intermediate[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(T_add_intermediate[v_i0, v_i1, v_i2, v_i3], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">reshape</span>(conv1_bias: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_reshape: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;op_pattern&quot;</span>: <span style=\"color: #008000\">2</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_reshape&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv1_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> conv1_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">reshape1</span>(conv2_bias: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_reshape: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;op_pattern&quot;</span>: <span style=\"color: #008000\">2</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_reshape&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv2_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)]\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">forward</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>reshape, (conv1_bias,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_conv2d_add_relu, (x, conv1_weight, lv1), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv3 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>reshape1, (conv2_bias,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_conv2d1_add1_relu1, (lv, conv2_weight, lv3), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transforms = [\n",
    "    # # Phase 2. Lowering to TIR, inherited TVM Relax's official \"zero\" pipeline\n",
    "    relax.transform.LegalizeOps(),\n",
    "    relax.transform.AnnotateTIROpPattern(),\n",
    "    relax.transform.FoldConstant(),\n",
    "    relax.transform.FuseOps(),\n",
    "    relax.transform.FuseTIR(),\n",
    "]\n",
    "\n",
    "new_mod = rconv_mod\n",
    "for t in transforms:\n",
    "    new_mod = t(new_mod)\n",
    "\n",
    "new_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c6d421f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# from tvm.script import ir as I\n",
       "# from tvm.script import tir as T\n",
       "# from tvm.script import relax as R\n",
       "\n",
       "@I.ir_module\n",
       "class Module:\n",
       "    @T.prim_func(private=True)\n",
       "    def fused_conv2d1_add1_relu1(relu: T.Buffer((T.int64(1), T.int64(32), T.int64(128), T.int64(128)), \"float32\"), conv2_weight: T.Buffer((T.int64(64), T.int64(32), T.int64(5), T.int64(5)), \"float32\"), lv3: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), \"float32\"), compute_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(128), T.int64(128)), \"float32\")):\n",
       "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        pad_temp = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(132), T.int64(132)))\n",
       "        conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(128), T.int64(128)))\n",
       "        T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(128), T.int64(128)))\n",
       "        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(32), T.int64(132), T.int64(132)):\n",
       "            with T.block(\"pad_temp\"):\n",
       "                v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
       "                T.reads(relu[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)])\n",
       "                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
       "                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(2) <= v_i2 and v_i2 < T.int64(130) and T.int64(2) <= v_i3 and v_i3 < T.int64(130), relu[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)], T.float32(0.0))\n",
       "        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(64), T.int64(128), T.int64(128), T.int64(32), T.int64(5), T.int64(5)):\n",
       "            with T.block(\"conv2d_nchw\"):\n",
       "                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap(\"SSSSRRR\", [nn, ff, yy, xx, rc, ry, rx])\n",
       "                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], conv2_weight[v_ff, v_rc, v_ry, v_rx])\n",
       "                T.writes(conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])\n",
       "                with T.init():\n",
       "                    conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0.0)\n",
       "                conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * conv2_weight[v_ff, v_rc, v_ry, v_rx]\n",
       "        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(128), T.int64(128)):\n",
       "            with T.block(\"T_add\"):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
       "                T.reads(conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv3[v_ax0, v_ax1, T.int64(0), T.int64(0)])\n",
       "                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv3[v_ax0, v_ax1, T.int64(0), T.int64(0)]\n",
       "        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(64), T.int64(128), T.int64(128)):\n",
       "            with T.block(\"compute\"):\n",
       "                v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
       "                T.reads(T_add_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                T.writes(compute_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                compute_intermediate[v_i0, v_i1, v_i2, v_i3] = T.max(T_add_intermediate[v_i0, v_i1, v_i2, v_i3], T.float32(0.0))\n",
       "\n",
       "    @T.prim_func(private=True)\n",
       "    def fused_conv2d_add_relu(x: T.Buffer((T.int64(1), T.int64(3), T.int64(128), T.int64(128)), \"float32\"), conv1_weight: T.Buffer((T.int64(32), T.int64(3), T.int64(5), T.int64(5)), \"float32\"), lv1: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), \"float32\"), compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(128), T.int64(128)), \"float32\")):\n",
       "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        pad_temp = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(132), T.int64(132)))\n",
       "        conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(128), T.int64(128)))\n",
       "        T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(128), T.int64(128)))\n",
       "        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(3), T.int64(132), T.int64(132)):\n",
       "            with T.block(\"pad_temp\"):\n",
       "                v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
       "                T.reads(x[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)])\n",
       "                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
       "                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(2) <= v_i2 and v_i2 < T.int64(130) and T.int64(2) <= v_i3 and v_i3 < T.int64(130), x[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)], T.float32(0.0))\n",
       "        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(32), T.int64(128), T.int64(128), T.int64(3), T.int64(5), T.int64(5)):\n",
       "            with T.block(\"conv2d_nchw\"):\n",
       "                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap(\"SSSSRRR\", [nn, ff, yy, xx, rc, ry, rx])\n",
       "                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], conv1_weight[v_ff, v_rc, v_ry, v_rx])\n",
       "                T.writes(conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])\n",
       "                with T.init():\n",
       "                    conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0.0)\n",
       "                conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * conv1_weight[v_ff, v_rc, v_ry, v_rx]\n",
       "        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(128), T.int64(128)):\n",
       "            with T.block(\"T_add\"):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
       "                T.reads(conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv1[v_ax0, v_ax1, T.int64(0), T.int64(0)])\n",
       "                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv1[v_ax0, v_ax1, T.int64(0), T.int64(0)]\n",
       "        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(32), T.int64(128), T.int64(128)):\n",
       "            with T.block(\"compute\"):\n",
       "                v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
       "                T.reads(T_add_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                T.writes(compute_intermediate[v_i0, v_i1, v_i2, v_i3])\n",
       "                compute_intermediate[v_i0, v_i1, v_i2, v_i3] = T.max(T_add_intermediate[v_i0, v_i1, v_i2, v_i3], T.float32(0.0))\n",
       "\n",
       "    @T.prim_func(private=True)\n",
       "    def reshape(conv1_bias: T.Buffer((T.int64(32),), \"float32\"), T_reshape: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), \"float32\")):\n",
       "        T.func_attr({\"op_pattern\": 2, \"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1)):\n",
       "            with T.block(\"T_reshape\"):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
       "                T.reads(conv1_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(32)])\n",
       "                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = conv1_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(32)]\n",
       "\n",
       "    @T.prim_func(private=True)\n",
       "    def reshape1(conv2_bias: T.Buffer((T.int64(64),), \"float32\"), T_reshape: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), \"float32\")):\n",
       "        T.func_attr({\"op_pattern\": 2, \"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(1), T.int64(1)):\n",
       "            with T.block(\"T_reshape\"):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
       "                T.reads(conv2_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(64)])\n",
       "                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = conv2_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(64)]\n",
       "\n",
       "    @R.function\n",
       "    def forward(x: R.Tensor((1, 3, 128, 128), dtype=\"float32\"), conv1_weight: R.Tensor((32, 3, 5, 5), dtype=\"float32\"), conv1_bias: R.Tensor((32,), dtype=\"float32\"), conv2_weight: R.Tensor((64, 32, 5, 5), dtype=\"float32\"), conv2_bias: R.Tensor((64,), dtype=\"float32\")) -> R.Tensor((1, 64, 128, 128), dtype=\"float32\"):\n",
       "        R.func_attr({\"num_input\": 1})\n",
       "        cls = Module\n",
       "        with R.dataflow():\n",
       "            lv1 = R.call_tir(cls.reshape, (conv1_bias,), out_sinfo=R.Tensor((1, 32, 1, 1), dtype=\"float32\"))\n",
       "            lv = R.call_tir(cls.fused_conv2d_add_relu, (x, conv1_weight, lv1), out_sinfo=R.Tensor((1, 32, 128, 128), dtype=\"float32\"))\n",
       "            lv3 = R.call_tir(cls.reshape1, (conv2_bias,), out_sinfo=R.Tensor((1, 64, 1, 1), dtype=\"float32\"))\n",
       "            gv = R.call_tir(cls.fused_conv2d1_add1_relu1, (lv, conv2_weight, lv3), out_sinfo=R.Tensor((1, 64, 128, 128), dtype=\"float32\"))\n",
       "            R.output(gv)\n",
       "        return gv"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tvm\n",
    "tvm.ir.module.IRModule\n",
    "new_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9284c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AllocationFinder import INSIGHT\n",
    "\n",
    "alloc_finder = AllocationFinder(new_mod)\n",
    "alloc_finder.walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdee2c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d007b287': MemBlock(x:d007b287, shape=(1, 3, 128, 128), dtype=float32, size=196608, origin=relax.input),\n",
       " 'ce6abce4': MemBlock(conv1_weight:ce6abce4, shape=(32, 3, 5, 5), dtype=float32, size=9600, origin=relax.input),\n",
       " '4523b3ab': MemBlock(conv1_bias:4523b3ab, shape=(32,), dtype=float32, size=128, origin=relax.input),\n",
       " '5e992f67': MemBlock(conv2_weight:5e992f67, shape=(64, 32, 5, 5), dtype=float32, size=204800, origin=relax.input),\n",
       " '56ee3aa3': MemBlock(conv2_bias:56ee3aa3, shape=(64,), dtype=float32, size=256, origin=relax.input),\n",
       " '516d9447': MemBlock(lv1:516d9447, shape=(1, 32, 1, 1), dtype=float32, size=128, origin=relax.call_tir.reshape),\n",
       " '32d0cbd3': MemBlock(lv:32d0cbd3, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=relax.call_tir.fused_conv2d_add_relu),\n",
       " 'a6dcc9de': MemBlock(lv3:a6dcc9de, shape=(1, 64, 1, 1), dtype=float32, size=256, origin=relax.call_tir.reshape1),\n",
       " '7abdc45b': MemBlock(gv:7abdc45b, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=relax.call_tir.fused_conv2d1_add1_relu1),\n",
       " 'de3aec92': MemBlock(pad_temp:de3aec92, shape=(1, 32, 132, 132), dtype=float32, size=2230272, origin=tir.fused_conv2d1_add1_relu1.pad_temp),\n",
       " 'f16efbd9': MemBlock(conv2d_nchw_intermediate:f16efbd9, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.conv2d_nchw_intermediate),\n",
       " 'f2dceb7f': MemBlock(T_add_intermediate:f2dceb7f, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.T_add_intermediate),\n",
       " 'd8d42eab': MemBlock(pad_temp:d8d42eab, shape=(1, 3, 132, 132), dtype=float32, size=209088, origin=tir.fused_conv2d_add_relu.pad_temp),\n",
       " 'eba01b42': MemBlock(conv2d_nchw_intermediate:eba01b42, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.conv2d_nchw_intermediate),\n",
       " 'd93b7024': MemBlock(T_add_intermediate:d93b7024, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.T_add_intermediate)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alloc_finder.id_to_memblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb20ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: forward\n",
      "    MemBlock(x:d007b287, shape=(1, 3, 128, 128), dtype=float32, size=196608, origin=relax.input)\n",
      "    MemBlock(conv1_weight:ce6abce4, shape=(32, 3, 5, 5), dtype=float32, size=9600, origin=relax.input)\n",
      "    MemBlock(conv1_bias:4523b3ab, shape=(32,), dtype=float32, size=128, origin=relax.input)\n",
      "    MemBlock(conv2_weight:5e992f67, shape=(64, 32, 5, 5), dtype=float32, size=204800, origin=relax.input)\n",
      "    MemBlock(conv2_bias:56ee3aa3, shape=(64,), dtype=float32, size=256, origin=relax.input)\n",
      "    MemBlock(lv1:516d9447, shape=(1, 32, 1, 1), dtype=float32, size=128, origin=relax.call_tir.reshape)\n",
      "    MemBlock(lv:32d0cbd3, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=relax.call_tir.fused_conv2d_add_relu)\n",
      "    MemBlock(lv3:a6dcc9de, shape=(1, 64, 1, 1), dtype=float32, size=256, origin=relax.call_tir.reshape1)\n",
      "    MemBlock(gv:7abdc45b, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=relax.call_tir.fused_conv2d1_add1_relu1)\n",
      "Function: fused_conv2d1_add1_relu1\n",
      "    MemBlock(pad_temp:de3aec92, shape=(1, 32, 132, 132), dtype=float32, size=2230272, origin=tir.fused_conv2d1_add1_relu1.pad_temp)\n",
      "    MemBlock(conv2d_nchw_intermediate:f16efbd9, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.conv2d_nchw_intermediate)\n",
      "    MemBlock(T_add_intermediate:f2dceb7f, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.T_add_intermediate)\n",
      "Function: fused_conv2d_add_relu\n",
      "    MemBlock(pad_temp:d8d42eab, shape=(1, 3, 132, 132), dtype=float32, size=209088, origin=tir.fused_conv2d_add_relu.pad_temp)\n",
      "    MemBlock(conv2d_nchw_intermediate:eba01b42, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.conv2d_nchw_intermediate)\n",
      "    MemBlock(T_add_intermediate:d93b7024, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.T_add_intermediate)\n"
     ]
    }
   ],
   "source": [
    "for func, mbs in alloc_finder.memblocks.items():\n",
    "    print(f\"Function: {func}\")\n",
    "    for mb in mbs:\n",
    "        print(\"   \", mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dafcb81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemBlock(pad_temp:d8d42eab, shape=(1, 3, 132, 132), dtype=float32, size=209088, origin=tir.fused_conv2d_add_relu.pad_temp) \n",
      "    - Depends on [MemBlock(x:d007b287, shape=(1, 3, 128, 128), dtype=float32, size=196608, origin=relax.input)]\n",
      "    - Links to [MemBlock(conv2d_nchw_intermediate:eba01b42, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.conv2d_nchw_intermediate)]\n",
      "MemBlock(conv2d_nchw_intermediate:eba01b42, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.conv2d_nchw_intermediate) \n",
      "    - Depends on [MemBlock(pad_temp:d8d42eab, shape=(1, 3, 132, 132), dtype=float32, size=209088, origin=tir.fused_conv2d_add_relu.pad_temp), MemBlock(conv1_weight:ce6abce4, shape=(32, 3, 5, 5), dtype=float32, size=9600, origin=relax.input)]\n",
      "    - Links to [MemBlock(T_add_intermediate:d93b7024, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.T_add_intermediate)]\n",
      "MemBlock(T_add_intermediate:d93b7024, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.T_add_intermediate) \n",
      "    - Depends on [MemBlock(conv2d_nchw_intermediate:eba01b42, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.conv2d_nchw_intermediate), MemBlock(lv1:516d9447, shape=(1, 32, 1, 1), dtype=float32, size=128, origin=relax.call_tir.reshape)]\n",
      "    - Links to [MemBlock(lv:32d0cbd3, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=relax.call_tir.fused_conv2d_add_relu)]\n"
     ]
    }
   ],
   "source": [
    "for mb in alloc_finder.memblocks[\"fused_conv2d_add_relu\"]:\n",
    "    print(f\"{mb} \")\n",
    "    print(f\"    - Depends on {mb.depends_on}\")\n",
    "    print(f\"    - Links to {mb.links_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9280af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemBlock(pad_temp:de3aec92, shape=(1, 32, 132, 132), dtype=float32, size=2230272, origin=tir.fused_conv2d1_add1_relu1.pad_temp) \n",
      "    - Depends on [MemBlock(lv:32d0cbd3, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=relax.call_tir.fused_conv2d_add_relu)]\n",
      "    - Links to [MemBlock(conv2d_nchw_intermediate:f16efbd9, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.conv2d_nchw_intermediate)]\n",
      "MemBlock(conv2d_nchw_intermediate:f16efbd9, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.conv2d_nchw_intermediate) \n",
      "    - Depends on [MemBlock(pad_temp:de3aec92, shape=(1, 32, 132, 132), dtype=float32, size=2230272, origin=tir.fused_conv2d1_add1_relu1.pad_temp), MemBlock(conv2_weight:5e992f67, shape=(64, 32, 5, 5), dtype=float32, size=204800, origin=relax.input)]\n",
      "    - Links to [MemBlock(T_add_intermediate:f2dceb7f, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.T_add_intermediate)]\n",
      "MemBlock(T_add_intermediate:f2dceb7f, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.T_add_intermediate) \n",
      "    - Depends on [MemBlock(conv2d_nchw_intermediate:f16efbd9, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.conv2d_nchw_intermediate), MemBlock(lv3:a6dcc9de, shape=(1, 64, 1, 1), dtype=float32, size=256, origin=relax.call_tir.reshape1)]\n",
      "    - Links to [MemBlock(gv:7abdc45b, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=relax.call_tir.fused_conv2d1_add1_relu1)]\n"
     ]
    }
   ],
   "source": [
    "for mb in alloc_finder.memblocks[\"fused_conv2d1_add1_relu1\"]:\n",
    "    print(f\"{mb} \")\n",
    "    print(f\"    - Depends on {mb.depends_on}\")\n",
    "    print(f\"    - Links to {mb.links_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14979644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemBlock(x:d007b287, shape=(1, 3, 128, 128), dtype=float32, size=196608, origin=relax.input) \n",
      "    - Depends on []\n",
      "    - Links to [MemBlock(pad_temp:d8d42eab, shape=(1, 3, 132, 132), dtype=float32, size=209088, origin=tir.fused_conv2d_add_relu.pad_temp)]\n",
      "MemBlock(conv1_weight:ce6abce4, shape=(32, 3, 5, 5), dtype=float32, size=9600, origin=relax.input) \n",
      "    - Depends on []\n",
      "    - Links to [MemBlock(conv2d_nchw_intermediate:eba01b42, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.conv2d_nchw_intermediate)]\n",
      "MemBlock(conv1_bias:4523b3ab, shape=(32,), dtype=float32, size=128, origin=relax.input) \n",
      "    - Depends on []\n",
      "    - Links to [MemBlock(lv1:516d9447, shape=(1, 32, 1, 1), dtype=float32, size=128, origin=relax.call_tir.reshape)]\n",
      "MemBlock(conv2_weight:5e992f67, shape=(64, 32, 5, 5), dtype=float32, size=204800, origin=relax.input) \n",
      "    - Depends on []\n",
      "    - Links to [MemBlock(conv2d_nchw_intermediate:f16efbd9, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.conv2d_nchw_intermediate)]\n",
      "MemBlock(conv2_bias:56ee3aa3, shape=(64,), dtype=float32, size=256, origin=relax.input) \n",
      "    - Depends on []\n",
      "    - Links to [MemBlock(lv3:a6dcc9de, shape=(1, 64, 1, 1), dtype=float32, size=256, origin=relax.call_tir.reshape1)]\n",
      "MemBlock(lv1:516d9447, shape=(1, 32, 1, 1), dtype=float32, size=128, origin=relax.call_tir.reshape) \n",
      "    - Depends on [MemBlock(conv1_bias:4523b3ab, shape=(32,), dtype=float32, size=128, origin=relax.input)]\n",
      "    - Links to [MemBlock(T_add_intermediate:d93b7024, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.T_add_intermediate)]\n",
      "MemBlock(lv:32d0cbd3, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=relax.call_tir.fused_conv2d_add_relu) \n",
      "    - Depends on [MemBlock(T_add_intermediate:d93b7024, shape=(1, 32, 128, 128), dtype=float32, size=2097152, origin=tir.fused_conv2d_add_relu.T_add_intermediate)]\n",
      "    - Links to [MemBlock(pad_temp:de3aec92, shape=(1, 32, 132, 132), dtype=float32, size=2230272, origin=tir.fused_conv2d1_add1_relu1.pad_temp)]\n",
      "MemBlock(lv3:a6dcc9de, shape=(1, 64, 1, 1), dtype=float32, size=256, origin=relax.call_tir.reshape1) \n",
      "    - Depends on [MemBlock(conv2_bias:56ee3aa3, shape=(64,), dtype=float32, size=256, origin=relax.input)]\n",
      "    - Links to [MemBlock(T_add_intermediate:f2dceb7f, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.T_add_intermediate)]\n",
      "MemBlock(gv:7abdc45b, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=relax.call_tir.fused_conv2d1_add1_relu1) \n",
      "    - Depends on [MemBlock(T_add_intermediate:f2dceb7f, shape=(1, 64, 128, 128), dtype=float32, size=4194304, origin=tir.fused_conv2d1_add1_relu1.T_add_intermediate)]\n",
      "    - Links to []\n"
     ]
    }
   ],
   "source": [
    "for mb in alloc_finder.memblocks[\"forward\"]:\n",
    "    print(f\"{mb} \")\n",
    "    print(f\"    - Depends on {mb.depends_on}\")\n",
    "    print(f\"    - Links to {mb.links_to}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3221d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MinizincGenerator import generate_minizinc_model\n",
    "\n",
    "\n",
    "mzn_code = generate_minizinc_model(alloc_finder)\n",
    "with open(\"../generated_mzn/model.mzn\", \"w\") as f:\n",
    "    f.write(mzn_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
